{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FloorSegmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1SDrmzjXdSweCwBwDKNlHNzAbsAKKq--V",
      "authorship_tag": "ABX9TyO8ggkOJJaDtYYdvUUwoKXU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/J0ec0des/FloorSegmentaion/blob/main/FloorSegmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hTdS9TJlKq-",
        "outputId": "fb4cbb18-de76-4e17-9798-0146286e75de"
      },
      "source": [
        "!pip install segmentation-models\n",
        "!pip install tensorflow==2.1.0\n",
        "!pip install keras==2.3.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: segmentation-models in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: efficientnet==1.0.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models) (1.0.0)\n",
            "Requirement already satisfied: image-classifiers==1.0.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models) (1.0.0)\n",
            "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.7/dist-packages (from segmentation-models) (1.0.8)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from efficientnet==1.0.0->segmentation-models) (0.16.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.5.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.6.3)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.1.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (7.1.2)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (3.2.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.4.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (1.15.0)\n",
            "Requirement already satisfied: tensorflow==2.1.0 in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (3.3.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.40.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.19.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.12.1)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.4.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.2.2)\n",
            "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (2.1.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.37.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (3.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (57.4.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.35.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==2.1.0) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: keras==2.3.1 in /usr/local/lib/python3.7/dist-packages (2.3.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.3.1) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK92kv2fRXPN",
        "outputId": "f0ae64bf-6493-4103-b009-bda06fe7453b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "sys.path.append('/content/gdrive/MyDrive/')\n",
        "import project_tests\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "eKkGGwYuRILi",
        "outputId": "4d4de8b6-baae-4ad7-eba5-1be5e7dc03b0"
      },
      "source": [
        "import os.path\n",
        "import tensorflow as tf\n",
        "import helper\n",
        "import warnings\n",
        "from distutils.version import LooseVersion\n",
        "import project_tests as tests\n",
        "\n",
        "num_classes = 2\n",
        "image_shape = (160, 576)\n",
        "EPOCHS = 40\n",
        "BATCH_SIZE = 16\n",
        "DROPOUT = 0.75\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "correct_label = tf.compat.v1.placeholder(tf.float32, [None, image_shape[0], image_shape[1], num_classes])\n",
        "learning_rate = tf.compat.v1.placeholder(tf.float32)\n",
        "keep_prob = tf.compat.v1.placeholder(tf.float32)\n",
        "\n",
        "# Check TensorFlow Version\n",
        "assert LooseVersion(tf.__version__) >= LooseVersion(\n",
        "    '1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\n",
        "print('TensorFlow Version: {}'.format(tf.__version__))\n",
        "\n",
        "# Check for a GPU\n",
        "if not tf.test.gpu_device_name():\n",
        "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
        "else:\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "\n",
        "\n",
        "def load_vgg(sess, vgg_path):\n",
        "    \"\"\"\n",
        "    Load Pretrained VGG Model into TensorFlow.\n",
        "    :param sess: TensorFlow Session\n",
        "    :param vgg_path: Path to vgg folder, containing \"variables/\" and \"saved_model.pb\"\n",
        "    :return: Tuple of Tensors from VGG model (image_input, keep_prob, layer3_out, layer4_out, layer7_out)\n",
        "    \"\"\"\n",
        "    # TODO: Implement function\n",
        "    #   Use tf.saved_model.loader.load to load the model and weights\n",
        "    model = tf.saved_model.loader.load(sess, ['vgg16'], vgg_path)\n",
        "    vgg_tag = 'vgg16'\n",
        "    vgg_input_tensor_name = 'image_input:0'\n",
        "    vgg_keep_prob_tensor_name = 'keep_prob:0'\n",
        "    vgg_layer3_out_tensor_name = 'layer3_out:0'\n",
        "    vgg_layer4_out_tensor_name = 'layer4_out:0'\n",
        "    vgg_layer7_out_tensor_name = 'layer7_out:0'\n",
        "\n",
        "    return vgg_input_tensor_name, vgg_keep_prob_tensor_name, vgg_layer3_out_tensor_name, vgg_layer4_out_tensor_name, vgg_layer7_out_tensor_name\n",
        "\n",
        "\n",
        "#tests.test_load_vgg(load_vgg, tf)\n",
        "\n",
        "\n",
        "def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes):\n",
        "    \"\"\"\n",
        "    Create the layers for a fully convolutional network.  Build skip-layers using the vgg layers.\n",
        "    :param vgg_layer3_out: TF Tensor for VGG Layer 3 output\n",
        "    :param vgg_layer4_out: TF Tensor for VGG Layer 4 output\n",
        "    :param vgg_layer7_out: TF Tensor for VGG Layer 7 output\n",
        "    :param num_classes: Number of classes to classify\n",
        "    :return: The Tensor for the last layer of output\n",
        "    \"\"\"\n",
        "    # TODO: Implement function\n",
        "    layer3, layer4, layer7 = vgg_layer3_out, vgg_layer4_out, vgg_layer7_out\n",
        "\n",
        "    # Apply 1x1 convolution in place of fully connected layer\n",
        "    fcn8 = tf.compat.v1.layers.conv2d(layer7, filters=num_classes, kernel_size=1, name=\"fcn8\")\n",
        "\n",
        "    # Upsample fcn8 with size depth=(4096?) to match size of layer 4 so that we can add skip connection with 4th layer\n",
        "    fcn9 = tf.compat.v1.layers.conv2d_transpose(fcn8, filters=layer4.get_shape().as_list()[-1],\n",
        "                                      kernel_size=4, strides=(2, 2), padding='SAME', name=\"fcn9\")\n",
        "\n",
        "    # Add a skip connection between current final layer fcn8 and 4th layer\n",
        "    fcn9_skip_connected = tf.add(fcn9, layer4, name=\"fcn9_plus_vgg_layer4\")\n",
        "\n",
        "    # Upsample again\n",
        "    fcn10 = tf.compat.v1.layers.conv2d_transpose(fcn9_skip_connected, filters=layer3.get_shape().as_list()[-1],\n",
        "                                       kernel_size=4, strides=(2, 2), padding='SAME', name=\"fcn10_conv2d\")\n",
        "\n",
        "    # Add skip connection\n",
        "    fcn10_skip_connected = tf.add(fcn10, layer3, name=\"fcn10_plus_vgg_layer3\")\n",
        "\n",
        "    # Upsample again\n",
        "    fcn11 = tf.compat.v1.layers.conv2d_transpose(fcn10_skip_connected, filters=num_classes,\n",
        "                                       kernel_size=16, strides=(8, 8), padding='SAME', name=\"fcn11\")\n",
        "\n",
        "    return fcn11\n",
        "\n",
        "\n",
        "tests.test_layers(layers)\n",
        "\n",
        "\n",
        "def optimize(nn_last_layer, correct_label, learning_rate, num_classes):\n",
        "    \"\"\"\n",
        "    Build the TensorFLow loss and optimizer operations.\n",
        "    :param nn_last_layer: TF Tensor of the last layer in the neural network\n",
        "    :param correct_label: TF Placeholder for the correct label image\n",
        "    :param learning_rate: TF Placeholder for the learning rate\n",
        "    :param num_classes: Number of classes to classify\n",
        "    :return: Tuple of (logits, train_op, cross_entropy_loss)\n",
        "    \"\"\"\n",
        "    # TODO: Implement function\n",
        "\n",
        "    # Reshape 4D tensors to 2D, each row represents a pixel, each column a class\n",
        "    logits = tf.reshape(nn_last_layer, (-1, num_classes), name=\"fcn_logits\")\n",
        "    correct_label_reshaped = tf.reshape(correct_label, (-1, num_classes))\n",
        "\n",
        "    # Calculate distance from actual labels using cross entropy\n",
        "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=correct_label_reshaped[:])\n",
        "    # Take mean for total loss\n",
        "    loss_op = tf.reduce_mean(cross_entropy, name=\"fcn_loss\")\n",
        "\n",
        "    # The model implements this operation to find the weights/parameters that would yield correct pixel labels\n",
        "    train_op = tf.optimizers.Adam(learning_rate=learning_rate).minimize(loss_op, var_list=None, name=\"fcn_train_op\")\n",
        "\n",
        "    return logits, train_op, loss_op\n",
        "\n",
        "\n",
        "tests.test_optimize(optimize)\n",
        "\n",
        "\n",
        "def train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input_image,\n",
        "             correct_label, keep_prob, learning_rate):\n",
        "    \"\"\"\n",
        "    Train neural network and print out the loss during training.\n",
        "    :param sess: TF Session\n",
        "    :param epochs: Number of epochs\n",
        "    :param batch_size: Batch size\n",
        "    :param get_batches_fn: Function to get batches of training data.  Call using get_batches_fn(batch_size)\n",
        "    :param train_op: TF Operation to train the neural network\n",
        "    :param cross_entropy_loss: TF Tensor for the amount of loss\n",
        "    :param input_image: TF Placeholder for input images\n",
        "    :param correct_label: TF Placeholder for label images\n",
        "    :param keep_prob: TF Placeholder for dropout keep probability\n",
        "    :param learning_rate: TF Placeholder for learning rate\n",
        "    \"\"\"\n",
        "    # TODO: Implement function\n",
        "    keep_prob_value = 0.5\n",
        "    learning_rate_value = 0.001\n",
        "    for epoch in range(epochs):\n",
        "        # Create function to get batches\n",
        "        total_loss = 0\n",
        "        for X_batch, gt_batch in get_batches_fn(batch_size):\n",
        "            loss, _ = sess.run([cross_entropy_loss, train_op],\n",
        "                               feed_dict={input_image: X_batch, correct_label: gt_batch,\n",
        "                                          keep_prob: keep_prob_value, learning_rate: learning_rate_value})\n",
        "\n",
        "            total_loss += loss;\n",
        "\n",
        "        print(\"EPOCH {} ...\".format(epoch + 1))\n",
        "        print(\"Loss = {:.3f}\".format(total_loss))\n",
        "        print()\n",
        "\n",
        "\n",
        "tests.test_train_nn(train_nn)\n",
        "\n",
        "\n",
        "def run():\n",
        "    num_classes = 2\n",
        "    image_shape = (160, 576)  # KITTI dataset uses 160x576 images\n",
        "    data_dir = './data'\n",
        "    runs_dir = './runs'\n",
        "    tests.test_for_kitti_dataset(data_dir)\n",
        "\n",
        "    # Download pretrained vgg model\n",
        "    helper.maybe_download_pretrained_vgg(data_dir)\n",
        "\n",
        "    # OPTIONAL: Train and Inference on the cityscapes dataset instead of the Kitti dataset.\n",
        "    # You'll need a GPU with at least 10 teraFLOPS to train on.\n",
        "    #  https://www.cityscapes-dataset.com/\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        # Path to vgg model\n",
        "        vgg_path = os.path.join(data_dir, 'vgg')\n",
        "        # Create function to get batches\n",
        "        get_batches_fn = helper.gen_batch_function(os.path.join(data_dir, 'data_road/training'), image_shape)\n",
        "\n",
        "        # OPTIONAL: Augment Images for better results\n",
        "        #  https://datascience.stackexchange.com/questions/5224/how-to-prepare-augment-images-for-neural-network\n",
        "\n",
        "        # TODO: Build NN using load_vgg, layers, and optimize function\n",
        "        # Returns the three layers, keep probability and input layer from the vgg architecture\n",
        "        image_input, keep_prob, layer3, layer4, layer7 = load_vgg(sess, vgg_path)\n",
        "\n",
        "        # The resulting network architecture from adding a decoder on top of the given vgg model\n",
        "        model_output = layers(layer3, layer4, layer7, num_classes)\n",
        "\n",
        "        # Returns the output logits, training operation and cost operation to be used\n",
        "        # - logits: each row represents a pixel, each column a class\n",
        "        # - train_op: function used to get the right parameters to the model to correctly label the pixels\n",
        "        # - cross_entropy_loss: function outputting the cost which we are minimizing, lower cost should yield higher accuracy\n",
        "        logits, train_op, cross_entropy_loss = optimize(model_output, correct_label, learning_rate, num_classes)\n",
        "\n",
        "        # Initialize all variables\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        sess.run(tf.local_variables_initializer())\n",
        "\n",
        "        print(\"Model build successful, starting training\")\n",
        "\n",
        "        # Train the neural network\n",
        "        train_nn(sess, EPOCHS, BATCH_SIZE, get_batches_fn,\n",
        "                 train_op, cross_entropy_loss, image_input,\n",
        "                 correct_label, keep_prob, learning_rate)\n",
        "\n",
        "        # Run the model with the test images and save each painted output image (roads painted green)\n",
        "        helper.save_inference_samples(runs_dir, data_dir, sess, image_shape, logits, keep_prob, image_input)\n",
        "        # TODO: Train NN using the train_nn function\n",
        "\n",
        "        # TODO: Save inference data using helper.save_inference_samples\n",
        "        #  helper.save_inference_samples(runs_dir, data_dir, sess, image_shape, logits, keep_prob, input_image)\n",
        "\n",
        "        # OPTIONAL: Apply the trained model to a video\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.1.0\n",
            "Default GPU Device: /device:GPU:0\n",
            "WARNING:tensorflow:From <ipython-input-3-798465e95016>:66: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From <ipython-input-3-798465e95016>:70: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "Tests Passed\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-798465e95016>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m \u001b[0mtests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_optimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/MyDrive/project_tests.py\u001b[0m in \u001b[0;36mfunc_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunc_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tests Passed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/MyDrive/project_tests.py\u001b[0m in \u001b[0;36mtest_optimize\u001b[0;34m(optimize)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mcorrect_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0m_assert_tensor_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Logits'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-798465e95016>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(nn_last_layer, correct_label, learning_rate, num_classes)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m# The model implements this operation to find the weights/parameters that would yield correct pixel labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fcn_train_op\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \"\"\"\n\u001b[1;32m    315\u001b[0m     grads_and_vars = self._compute_gradients(\n\u001b[0;32m--> 316\u001b[0;31m         loss, var_list=var_list, grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m       \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mwatch\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_pywrap_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIsTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIsVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         raise ValueError(\"Passed in object of type {}, not tf.Tensor\".format(\n\u001b[0;32m--> 862\u001b[0;31m             type(t)))\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbackprop_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIsTrainable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         logging.log_first_n(\n",
            "\u001b[0;31mValueError\u001b[0m: Passed in object of type <class 'NoneType'>, not tf.Tensor"
          ]
        }
      ]
    }
  ]
}